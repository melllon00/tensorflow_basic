{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## - 학습 3은 학습 2를 기반으로 진행합니다 (중반부터 학습 3 진행)\n",
    "\n",
    "## tensorflow 모델 생성하기\n",
    "**출처 ** <br>\n",
    "- https://www.youtube.com/watch?v=NZz2yq0G5LU\n",
    "- http://bcho.tistory.com/1154"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from tensorflow.examples.tutorials.mnist import input_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### input과 label 데이터 설정\n",
    "** 먼저 간단한 모델을 살펴보자 ** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "input_data = [[1,5,3,7,8,10,12],\n",
    "              [5,8,10,3,9,7,1]]\n",
    "label_data = [[0,0,0,1,0],\n",
    "              [1,0,0,0,0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "INPUT_SIZE = 7 \n",
    "HIDDEN_SIZE_1 = 10\n",
    "HIDDEN_SIZE_2 = 8\n",
    "CLASSES = 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** shape 에서 첫 번째 요소는 batch의 크기. 일반적으로 잘 모르기 때문에 None으로 써도 된다. **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x = tf.placeholder(tf.float32, shape = [None, INPUT_SIZE], name=\"x\" )\n",
    "y = tf.placeholder(tf.float32, shape = [None, CLASSES], name=\"y\" )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tensor_map = {x : input_data, y : label_data}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** input weight 정의 ** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "weight_hidden_1 = tf.Variable( tf.truncated_normal(shape=[INPUT_SIZE, HIDDEN_SIZE_1]) , dtype=tf.float32 ) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** bias 정의 ** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bias_hidden_1 = tf.Variable( tf.zeros(shape=[HIDDEN_SIZE_1]) , dtype=tf.float32 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'MatMul:0' shape=(?, 10) dtype=float32>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.matmul( x , weight_hidden_1 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"x:0\", shape=(?, 7), dtype=float32)\n",
      "Tensor(\"Variable/read:0\", shape=(7, 10), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "print (x)\n",
    "print (weight_hidden_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "hidden_1 = tf.matmul( x , weight_hidden_1 ) + bias_hidden_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "weight_hidden_2 = tf.Variable( tf.truncated_normal(shape=[HIDDEN_SIZE_1, HIDDEN_SIZE_2]) , dtype=tf.float32 ) \n",
    "bias_hidden_2 = tf.Variable( tf.zeros(shape=[HIDDEN_SIZE_2]) , dtype=tf.float32 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "hidden_2 = tf.matmul( hidden_1 , weight_hidden_2 ) + bias_hidden_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "weight_output = tf.Variable( tf.truncated_normal(shape=[HIDDEN_SIZE_2, CLASSES]) , dtype=tf.float32 ) \n",
    "bias_output = tf.Variable( tf.zeros(shape=[CLASSES]) , dtype=tf.float32 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "answer = tf.matmul( hidden_2 , weight_output ) + bias_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** 위의 내용을 한 번에 모아서 해보자 ** \n",
    "\n",
    "신경망을 미리 설계해 놓고 아래에서 연산을 수행하여 답을 도출해낸다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "weight_hidden_1 = tf.Variable( tf.truncated_normal(shape=[INPUT_SIZE, HIDDEN_SIZE_1]) , dtype=tf.float32 ) \n",
    "bias_hidden_1 = tf.Variable( tf.zeros(shape=[HIDDEN_SIZE_1]) , dtype=tf.float32 )\n",
    "\n",
    "weight_hidden_2 = tf.Variable( tf.truncated_normal(shape=[HIDDEN_SIZE_1, HIDDEN_SIZE_2]) , dtype=tf.float32 ) \n",
    "bias_hidden_2 = tf.Variable( tf.zeros(shape=[HIDDEN_SIZE_2]) , dtype=tf.float32 )\n",
    "\n",
    "weight_output = tf.Variable( tf.truncated_normal(shape=[HIDDEN_SIZE_2, CLASSES]) , dtype=tf.float32 ) \n",
    "bias_output = tf.Variable( tf.zeros(shape=[CLASSES]) , dtype=tf.float32 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "hidden_1 = tf.matmul( x , weight_hidden_1 ) + bias_hidden_1\n",
    "hidden_2 = tf.matmul( hidden_1 , weight_hidden_2 ) + bias_hidden_2\n",
    "answer = tf.matmul( hidden_2 , weight_output ) + bias_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 구조\n",
    "    Input -  ㅁ  - ㅁ  - ㅁ - ㅁ   - ㅁ - ㅁ  - answer\n",
    "             |     |    |    |     |    |\n",
    "             w1   b2    w2   b2    wo   bo\n",
    "             \n",
    "    \n",
    "    인풋   히든1  2  결과  \n",
    "\n",
    "    7     10    8   5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"x:0\", shape=(?, 7), dtype=float32)\n",
      "Tensor(\"add_3:0\", shape=(?, 10), dtype=float32)\n",
      "Tensor(\"add_4:0\", shape=(?, 8), dtype=float32)\n",
      "Tensor(\"add_5:0\", shape=(?, 5), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "print (x)\n",
    "print (hidden_1)\n",
    "print (hidden_2)\n",
    "print (answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"Variable_6/read:0\", shape=(7, 10), dtype=float32)\n",
      "Tensor(\"Variable_7/read:0\", shape=(10,), dtype=float32)\n",
      "Tensor(\"Variable_8/read:0\", shape=(10, 8), dtype=float32)\n",
      "Tensor(\"Variable_9/read:0\", shape=(8,), dtype=float32)\n",
      "Tensor(\"Variable_10/read:0\", shape=(8, 5), dtype=float32)\n",
      "Tensor(\"Variable_11/read:0\", shape=(5,), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "print (weight_hidden_1)\n",
    "print (bias_hidden_1)\n",
    "\n",
    "print (weight_hidden_2)\n",
    "print (bias_hidden_2)\n",
    "\n",
    "print (weight_output)\n",
    "print (bias_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** sigmoid  함수 적용 법 ** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "weight_hidden_1 = tf.Variable( tf.truncated_normal(shape=[INPUT_SIZE, HIDDEN_SIZE_1]) , dtype=tf.float32 ) \n",
    "bias_hidden_1 = tf.Variable( tf.zeros(shape=[HIDDEN_SIZE_1]) , dtype=tf.float32 )\n",
    "hidden_1 = tf.sigmoid(tf.matmul( x , weight_hidden_1 ) + bias_hidden_1)\n",
    "\n",
    "weight_hidden_2 = tf.Variable( tf.truncated_normal(shape=[HIDDEN_SIZE_1, HIDDEN_SIZE_2]) , dtype=tf.float32 ) \n",
    "bias_hidden_2 = tf.Variable( tf.zeros(shape=[HIDDEN_SIZE_2]) , dtype=tf.float32 )\n",
    "hidden_2 = tf.sigmoid(tf.matmul( hidden_1 , weight_hidden_2 ) + bias_hidden_2)\n",
    "\n",
    "weight_output = tf.Variable( tf.truncated_normal(shape=[HIDDEN_SIZE_2, CLASSES]) , dtype=tf.float32 ) \n",
    "bias_output = tf.Variable( tf.zeros(shape=[CLASSES]) , dtype=tf.float32 )\n",
    "\n",
    "answer = tf.sigmoid(tf.matmul( hidden_2 , weight_output ) + bias_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** cost 엔트로피 ** ( tip : 매뉴얼을 항상 자주 보자 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cost = -y*tf.log(answer)-(1-y)*tf.log((1-answer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.12725647,  0.94917715,  1.55667984,  0.18586321,  1.14814878],\n",
       "       [ 3.00087166,  0.76838911,  1.09240842,  1.86383474,  0.65159225]], dtype=float32)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sess = tf.Session()\n",
    "init = tf.global_variables_initializer()\n",
    "sess.run(init)\n",
    "sess.run(cost, feed_dict = tensor_map)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** recude sum 과 reduce mean 사용해보기 **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9.898283"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cost = tf.reduce_sum(-y*tf.log(answer)-(1-y)*tf.log((1-answer)))\n",
    "\n",
    "sess = tf.Session()\n",
    "init = tf.global_variables_initializer()\n",
    "sess.run(init)\n",
    "sess.run(cost, feed_dict = tensor_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.6460587"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cost = tf.reduce_mean(-y*tf.log(answer)-(1-y)*tf.log((1-answer)))\n",
    "\n",
    "sess = tf.Session()\n",
    "init = tf.global_variables_initializer()\n",
    "sess.run(init)\n",
    "sess.run(cost, feed_dict = tensor_map)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** 수행 ** \n",
    "- cost를 최소화 하는 방향으로 최적화 해야 되니까 gradient descent가 필요.\n",
    "- learning rate 사용\n",
    "\n",
    "\n",
    "\n",
    "#### 조대협님의 블로그에서 가져온 내용 (Mnist data를 기준으로 한다. 자세한 내용은 링크 참조)\n",
    "\n",
    "우리가 구하고자 하는 값은 x 값으로 학습을 시켜서 0~9를 가장 잘 구별해내는 W와 b의 값을 찾는 일이다. \n",
    "\n",
    "여기서 코드를 주의깊게 봤다면 하나의 의문이 생길것이다.\n",
    "\n",
    "x의 데이타는 총 55000개로, 55000x784 행렬이 되고, W는 784x10 행렬이다. 이 둘을 곱하면, 55000x10 행렬이 되는데, b는 1x10 행렬로 차원이 달라서 합이 되지 않는다. \n",
    "\n",
    "텐서플로우와 파이썬에서는 이렇게 차원이 다른 행렬을 큰 행렬의 크기로 늘려주는 기능이 있는데, 이를 브로드 캐스팅이라고 한다. (브로드 캐스팅 개념 참고 - http://bcho.tistory.com/1153)\n",
    "\n",
    "브로드 캐스팅에 의해서 b는 55000x10 사이즈로 자동으로 늘어나고 각 행에는 첫행과 같은 데이타들로 채워지게 된다. \n",
    "\n",
    "소프트맥스 알고리즘을 이해하고 사용해도 좋지만, 텐서플로우에는 이미 tf.nn.softmax 라는 함수로 만들어져 있고, 대부분 많이 알려진 머신러닝 모델들은 샘플들이 많이 있기 때문에, 대략적인 원리만 이해하고 가져다 쓰는 것을 권장한다.\n",
    "\n",
    "보통 모델을 다 이해하려고 하다가 수학에서 부딪혀서 포기하는 경우가 많은데, 디테일한 모델을 이해하기 힘들면, 그냥 함수나 예제코드를 가져다 쓰는 방법으로 접근하자. \n",
    "\n",
    "우리가 일반적인 프로그래밍에서도 해쉬테이블이나 트리와 같은 자료구조에 대해서 대략적인 개념만 이해하고 미리 정의된 라이브러리를 사용하지 직접 해쉬 테이블등을 구현하는 경우는 드물다.\n",
    "\n",
    "\n",
    "### 코스트(비용) 함수\n",
    "\n",
    "이 소프트맥스 함수에 대한 코스트 함수는 크로스엔트로피 (Cross entropy) 함수의 평균을 이용하는데, 복잡한 산식 없이 그냥 외워서 쓰자. \n",
    "다행이도 크로스엔트로피 함수역시 함수로 구현이 되어있다.\n",
    "\n",
    "** Cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(tf.matmul(x, W) + b, y_)) ** \n",
    "\n",
    "가설에 의해 계산된 값 y를 넣지 않고 tf.matmul(x, W) + b 를 넣은 이유는  tf.nn.softmax_cross_entropy_with_logits 함수 자체가 softmax를 포함하기 때문이다. y_은 학습을 위해서 입력된 값이다.  \n",
    "\n",
    "\n",
    "-> 코스트 함수로 위 함수는 써 본 적은 없고, 자세한 내용은 저도 잘 모르겠으나 일단 사용해보겠습니다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[None, 0.93857777]\n",
      "Step :  0\n",
      "[None, 0.92473018]\n",
      "Step :  1\n",
      "[None, 0.91232836]\n",
      "Step :  2\n",
      "[None, 0.90098572]\n",
      "Step :  3\n",
      "[None, 0.8904134]\n",
      "Step :  4\n",
      "[None, 0.88041723]\n",
      "Step :  5\n",
      "[None, 0.8708688]\n",
      "Step :  6\n",
      "[None, 0.86168253]\n",
      "Step :  7\n",
      "[None, 0.85279971]\n",
      "Step :  8\n",
      "[None, 0.84417838]\n",
      "Step :  9\n"
     ]
    }
   ],
   "source": [
    "Learning_Rate = 0.05\n",
    "\n",
    "cost = tf.reduce_mean(-y*tf.log(answer)-(1-y)*tf.log((1-answer)))\n",
    "train = tf.train.GradientDescentOptimizer(Learning_Rate).minimize(cost)\n",
    "\n",
    "sess = tf.Session()\n",
    "init = tf.global_variables_initializer()\n",
    "sess.run(init)\n",
    "\n",
    "for i in range(10):\n",
    "    print (sess.run([train, cost], feed_dict = tensor_map))\n",
    "    print (\"Step : \",i )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** 학습 100 번 수행 ** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[None, 0.83578843]\n",
      "Step :  0\n",
      "[None, 0.82760656]\n",
      "Step :  1\n",
      "[None, 0.81961554]\n",
      "Step :  2\n",
      "[None, 0.81180131]\n",
      "Step :  3\n",
      "[None, 0.80415261]\n",
      "Step :  4\n",
      "[None, 0.7966603]\n",
      "Step :  5\n",
      "[None, 0.78931677]\n",
      "Step :  6\n",
      "[None, 0.78211546]\n",
      "Step :  7\n",
      "[None, 0.77505082]\n",
      "Step :  8\n",
      "[None, 0.76811802]\n",
      "Step :  9\n",
      "[None, 0.76131272]\n",
      "Step :  10\n",
      "[None, 0.75463122]\n",
      "Step :  11\n",
      "[None, 0.74806988]\n",
      "Step :  12\n",
      "[None, 0.74162591]\n",
      "Step :  13\n",
      "[None, 0.73529625]\n",
      "Step :  14\n",
      "[None, 0.72907853]\n",
      "Step :  15\n",
      "[None, 0.72297019]\n",
      "Step :  16\n",
      "[None, 0.71696913]\n",
      "Step :  17\n",
      "[None, 0.71107304]\n",
      "Step :  18\n",
      "[None, 0.70528018]\n",
      "Step :  19\n",
      "[None, 0.69958848]\n",
      "Step :  20\n",
      "[None, 0.69399625]\n",
      "Step :  21\n",
      "[None, 0.68850154]\n",
      "Step :  22\n",
      "[None, 0.68310297]\n",
      "Step :  23\n",
      "[None, 0.67779869]\n",
      "Step :  24\n",
      "[None, 0.67258716]\n",
      "Step :  25\n",
      "[None, 0.66746688]\n",
      "Step :  26\n",
      "[None, 0.66243613]\n",
      "Step :  27\n",
      "[None, 0.65749365]\n",
      "Step :  28\n",
      "[None, 0.65263784]\n",
      "Step :  29\n",
      "[None, 0.6478672]\n",
      "Step :  30\n",
      "[None, 0.64318025]\n",
      "Step :  31\n",
      "[None, 0.63857567]\n",
      "Step :  32\n",
      "[None, 0.63405198]\n",
      "Step :  33\n",
      "[None, 0.62960768]\n",
      "Step :  34\n",
      "[None, 0.62524152]\n",
      "Step :  35\n",
      "[None, 0.62095201]\n",
      "Step :  36\n",
      "[None, 0.61673772]\n",
      "Step :  37\n",
      "[None, 0.61259735]\n",
      "Step :  38\n",
      "[None, 0.60852963]\n",
      "Step :  39\n",
      "[None, 0.60453308]\n",
      "Step :  40\n",
      "[None, 0.60060638]\n",
      "Step :  41\n",
      "[None, 0.59674823]\n",
      "Step :  42\n",
      "[None, 0.59295738]\n",
      "Step :  43\n",
      "[None, 0.5892325]\n",
      "Step :  44\n",
      "[None, 0.5855723]\n",
      "Step :  45\n",
      "[None, 0.58197552]\n",
      "Step :  46\n",
      "[None, 0.57844102]\n",
      "Step :  47\n",
      "[None, 0.57496727]\n",
      "Step :  48\n",
      "[None, 0.57155347]\n",
      "Step :  49\n",
      "[None, 0.5681982]\n",
      "Step :  50\n",
      "[None, 0.56490028]\n",
      "Step :  51\n",
      "[None, 0.56165862]\n",
      "Step :  52\n",
      "[None, 0.55847204]\n",
      "Step :  53\n",
      "[None, 0.55533952]\n",
      "Step :  54\n",
      "[None, 0.5522598]\n",
      "Step :  55\n",
      "[None, 0.54923195]\n",
      "Step :  56\n",
      "[None, 0.54625487]\n",
      "Step :  57\n",
      "[None, 0.54332745]\n",
      "Step :  58\n",
      "[None, 0.54044878]\n",
      "Step :  59\n",
      "[None, 0.53761774]\n",
      "Step :  60\n",
      "[None, 0.53483337]\n",
      "Step :  61\n",
      "[None, 0.53209484]\n",
      "Step :  62\n",
      "[None, 0.529401]\n",
      "Step :  63\n",
      "[None, 0.5267511]\n",
      "Step :  64\n",
      "[None, 0.52414405]\n",
      "Step :  65\n",
      "[None, 0.52157915]\n",
      "Step :  66\n",
      "[None, 0.51905525]\n",
      "Step :  67\n",
      "[None, 0.51657188]\n",
      "Step :  68\n",
      "[None, 0.51412785]\n",
      "Step :  69\n",
      "[None, 0.51172256]\n",
      "Step :  70\n",
      "[None, 0.50935507]\n",
      "Step :  71\n",
      "[None, 0.50702471]\n",
      "Step :  72\n",
      "[None, 0.5047307]\n",
      "Step :  73\n",
      "[None, 0.50247216]\n",
      "Step :  74\n",
      "[None, 0.50024849]\n",
      "Step :  75\n",
      "[None, 0.49805889]\n",
      "Step :  76\n",
      "[None, 0.49590269]\n",
      "Step :  77\n",
      "[None, 0.49377912]\n",
      "Step :  78\n",
      "[None, 0.49168769]\n",
      "Step :  79\n",
      "[None, 0.48962754]\n",
      "Step :  80\n",
      "[None, 0.48759818]\n",
      "Step :  81\n",
      "[None, 0.48559889]\n",
      "Step :  82\n",
      "[None, 0.48362908]\n",
      "Step :  83\n",
      "[None, 0.48168811]\n",
      "Step :  84\n",
      "[None, 0.47977543]\n",
      "Step :  85\n",
      "[None, 0.47789055]\n",
      "Step :  86\n",
      "[None, 0.47603279]\n",
      "Step :  87\n",
      "[None, 0.47420159]\n",
      "Step :  88\n",
      "[None, 0.47239646]\n",
      "Step :  89\n",
      "[None, 0.47061691]\n",
      "Step :  90\n",
      "[None, 0.46886238]\n",
      "Step :  91\n",
      "[None, 0.46713242]\n",
      "Step :  92\n",
      "[None, 0.4654265]\n",
      "Step :  93\n",
      "[None, 0.4637441]\n",
      "Step :  94\n",
      "[None, 0.46208486]\n",
      "Step :  95\n",
      "[None, 0.46044827]\n",
      "Step :  96\n",
      "[None, 0.45883384]\n",
      "Step :  97\n",
      "[None, 0.45724124]\n",
      "Step :  98\n",
      "[None, 0.45567003]\n",
      "Step :  99\n"
     ]
    }
   ],
   "source": [
    "for i in range(100):\n",
    "    print (sess.run([train, cost], feed_dict = tensor_map))\n",
    "    print (\"Step : \",i )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sess.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    " <br><br>\n",
    "\n",
    "## 학습 3 시작 \n",
    "\n",
    "### cost 에서 reduce_mean 을 제거하고 수행해보자\n",
    "\n",
    "** reduce_mean은 모든 차원의 값을 제거하고 나서 하나의 값으로 나타낸다 ** \n",
    "\n",
    "    1 2 3 4\n",
    "    5 6 7 8 \n",
    "\n",
    "reduce_mean은 위와 같은 배열이 있으면 (1+2+3+4+5+6+7+8)/8 연산을 통해서 모든 차원을 제거하고 하나의 값만 출력해준다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1.0048455   0.84496111  2.10641766  0.17243485  0.44292724]\n",
      " [ 0.53914297  1.00608134  1.41389239  1.96921289  0.61158365]]\n",
      "\n",
      "[[ 1.0048455   0.84496111  2.10641766  0.17243485  0.44292724]\n",
      " [ 0.53914297  1.00608134  1.41389239  1.96921289  0.61158365]]\n",
      "[[ 1.0048455   0.84496111  2.10641766  0.17243485  0.44292724]\n",
      " [ 0.53914297  1.00608134  1.41389239  1.96921289  0.61158365]]\n",
      "[[ 1.0048455   0.84496111  2.10641766  0.17243485  0.44292724]\n",
      " [ 0.53914297  1.00608134  1.41389239  1.96921289  0.61158365]]\n",
      "[[ 1.0048455   0.84496111  2.10641766  0.17243485  0.44292724]\n",
      " [ 0.53914297  1.00608134  1.41389239  1.96921289  0.61158365]]\n",
      "[[ 1.0048455   0.84496111  2.10641766  0.17243485  0.44292724]\n",
      " [ 0.53914297  1.00608134  1.41389239  1.96921289  0.61158365]]\n",
      "[[ 1.0048455   0.84496111  2.10641766  0.17243485  0.44292724]\n",
      " [ 0.53914297  1.00608134  1.41389239  1.96921289  0.61158365]]\n",
      "[[ 1.0048455   0.84496111  2.10641766  0.17243485  0.44292724]\n",
      " [ 0.53914297  1.00608134  1.41389239  1.96921289  0.61158365]]\n",
      "[[ 1.0048455   0.84496111  2.10641766  0.17243485  0.44292724]\n",
      " [ 0.53914297  1.00608134  1.41389239  1.96921289  0.61158365]]\n",
      "[[ 1.0048455   0.84496111  2.10641766  0.17243485  0.44292724]\n",
      " [ 0.53914297  1.00608134  1.41389239  1.96921289  0.61158365]]\n",
      "[[ 1.0048455   0.84496111  2.10641766  0.17243485  0.44292724]\n",
      " [ 0.53914297  1.00608134  1.41389239  1.96921289  0.61158365]]\n"
     ]
    }
   ],
   "source": [
    "Learning_Rate = 0.05\n",
    "\n",
    "cost = -y*tf.log(answer)-(1-y)*tf.log((1-answer))\n",
    "train = tf.train.GradientDescentOptimizer(Learning_Rate).minimize(cost)\n",
    "\n",
    "sess = tf.Session()\n",
    "init = tf.global_variables_initializer()\n",
    "sess.run(init)\n",
    "\n",
    "loss = sess.run(cost, feed_dict = tensor_map)\n",
    "print (loss) \n",
    "print ()    \n",
    "\n",
    "for i in range(10):\n",
    "    loss = sess.run(cost, feed_dict = tensor_map)\n",
    "    print (loss)\n",
    "\n",
    "sess.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**   https://www.tensorflow.org/api_docs/python/tf/reduce_mean 에서 확인해보세요  ** \n",
    "\n",
    "** 예시 (For example): ** "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "    x = tf.constant([[1., 1.], [2., 2.]])\n",
    "    tf.reduce_mean(x)  # 1.5\n",
    "    tf.reduce_mean(x, 0)  # [1.5, 1.5]\n",
    "    tf.reduce_mean(x, 1)  # [1.,  2.]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** 비교하기 쉽게 출력 해보자 ** \n",
    "\n",
    "- 차원의 차이가 아래와 같이 난다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.26175556  0.58373004  0.60565728  0.30459416  0.06072804]\n",
      " [ 0.99035496  0.47694016  1.10001564  2.00055599  0.01510703]]\n",
      "[ 1.81646514  4.58297396]\n"
     ]
    }
   ],
   "source": [
    "Learning_Rate = 0.05\n",
    "\n",
    "cost_ = -y*tf.log(answer)-(1-y)*tf.log((1-answer))\n",
    "cost = tf.reduce_sum(cost_, reduction_indices=1)\n",
    "train = tf.train.GradientDescentOptimizer(Learning_Rate).minimize(cost)\n",
    "\n",
    "sess = tf.Session()\n",
    "init = tf.global_variables_initializer()\n",
    "sess.run(init) \n",
    "\n",
    "loss_, loss = sess.run([cost_, cost], feed_dict = tensor_map)\n",
    "print (loss_)\n",
    "print (loss)\n",
    "    \n",
    "sess.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "- 배열에서 원소 하나 하나는 함수와 가지는 loss를 의미한다\n",
    "\n",
    "- 따라서, 함수 하나와 모든 점들간의 loss를 보기 위해서 모든 파라미터에 대하여 평균을 구하거나 합을 구하는 reduce를 사용"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 위에서 reduce_sum 을 구했으니까, reduce_mean이 필요\n",
    "\n",
    "- reduce_mean을 해주면 차원이 또 줄어든다 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "3.04245\n",
      "100\n",
      "2.89862\n",
      "200\n",
      "2.76912\n",
      "300\n",
      "2.65249\n",
      "400\n",
      "2.54735\n",
      "500\n",
      "2.45231\n",
      "600\n",
      "2.3659\n",
      "700\n",
      "2.28701\n",
      "800\n",
      "2.21607\n",
      "900\n",
      "2.15454\n"
     ]
    }
   ],
   "source": [
    "Learning_Rate = 0.05\n",
    "\n",
    "cost_ = -y*tf.log(answer)-(1-y)*tf.log((1-answer))\n",
    "cost = tf.reduce_sum(cost_, reduction_indices=1)\n",
    "cost = tf.reduce_mean(cost, reduction_indices=0)\n",
    "train = tf.train.GradientDescentOptimizer(Learning_Rate).minimize(cost)\n",
    "\n",
    "sess = tf.Session()\n",
    "init = tf.global_variables_initializer()\n",
    "sess.run(init) \n",
    "\n",
    "for i in range(1000):\n",
    "    if i%100 == 0:\n",
    "        _ , loss = sess.run([train, cost], feed_dict = tensor_map)\n",
    "        print (i)\n",
    "        print (loss)\n",
    "    \n",
    "sess.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### accuracy를 구해보자!! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Learning_Rate = 0.05\n",
    "\n",
    "cost_ = -y*tf.log(answer)-(1-y)*tf.log((1-answer))\n",
    "cost = tf.reduce_sum(cost_, reduction_indices=1)\n",
    "cost = tf.reduce_mean(cost, reduction_indices=0)\n",
    "train = tf.train.GradientDescentOptimizer(Learning_Rate).minimize(cost)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** train 해서 나온 answer 중에서 가장 큰 것과, label 과 비교한다. ** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "comp_pred = tf.equal( tf.arg_max(answer, 1), tf.arg_max(y, 1) )\n",
    "accuracy = tf.reduce_mean(tf.cast(comp_pred, tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "4.98481\n",
      "0.5\n",
      "\n",
      "100\n",
      "1.39882\n",
      "1.0\n",
      "\n",
      "200\n",
      "1.07151\n",
      "1.0\n",
      "\n",
      "300\n",
      "0.745282\n",
      "1.0\n",
      "\n",
      "400\n",
      "0.510762\n",
      "1.0\n",
      "\n",
      "500\n",
      "0.371681\n",
      "1.0\n",
      "\n",
      "600\n",
      "0.285704\n",
      "1.0\n",
      "\n",
      "700\n",
      "0.229145\n",
      "1.0\n",
      "\n",
      "800\n",
      "0.18983\n",
      "1.0\n",
      "\n",
      "900\n",
      "0.161219\n",
      "1.0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sess = tf.Session()\n",
    "init = tf.global_variables_initializer()\n",
    "sess.run(init) \n",
    "\n",
    "for i in range(1000):\n",
    "    _ , loss, acc = sess.run([train, cost, accuracy], feed_dict = tensor_map)\n",
    "    if i%100 == 0: \n",
    "        print (i)\n",
    "        print (loss)\n",
    "        print (acc)\n",
    "        print ()\n",
    "    \n",
    "sess.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 텐서플로우는 체크포인트에 weight 정보를 넣는다. test 모델에서 파일에 있는 weight를 가져온다. \n",
    "- weight 의 아래 쪽에 saver를 만든다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "weight_hidden_1 = tf.Variable( tf.truncated_normal(shape=[INPUT_SIZE, HIDDEN_SIZE_1]) , dtype=tf.float32, name=\"weight_hidden_1\" ) \n",
    "bias_hidden_1 = tf.Variable( tf.zeros(shape=[HIDDEN_SIZE_1]) , dtype=tf.float32, name = \"bias_hidden_1\" )\n",
    "\n",
    "weight_hidden_2 = tf.Variable( tf.truncated_normal(shape=[HIDDEN_SIZE_1, HIDDEN_SIZE_2]) , dtype=tf.float32, name=\"weight_hidden_2\" ) \n",
    "bias_hidden_2 = tf.Variable( tf.zeros(shape=[HIDDEN_SIZE_2]) , dtype=tf.float32, name = \"bias_hidden_2\" )\n",
    "\n",
    "weight_output = tf.Variable( tf.truncated_normal(shape=[HIDDEN_SIZE_2, CLASSES]) , dtype=tf.float32, name=\"weight_output\" ) \n",
    "bias_output = tf.Variable( tf.zeros(shape=[CLASSES]) , dtype=tf.float32, name = \"bias_output\" )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** 아래의 영역을 추가해준다 ** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "param_list = [weight_hidden_1, bias_hidden_1, weight_hidden_2, bias_hidden_2, weight_output, bias_output ]\n",
    "saver = tf.train.Saver(param_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "hidden_1 = tf.matmul( x , weight_hidden_1 , name=\"hidden_1\" ) + bias_hidden_1\n",
    "hidden_2 = tf.matmul( hidden_1 , weight_hidden_2, name=\"hidden_2\" ) + bias_hidden_2\n",
    "answer = tf.matmul( hidden_2 , weight_output, name=\"answer\" ) + bias_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** saver.save 함수를 통하여 파일이 생성됨  ** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "nan\n",
      "0.0\n",
      "100\n",
      "nan\n",
      "0.0\n",
      "200\n",
      "nan\n",
      "0.0\n",
      "300\n",
      "nan\n",
      "0.0\n",
      "400\n",
      "nan\n",
      "0.0\n",
      "500\n",
      "nan\n",
      "0.0\n",
      "600\n",
      "nan\n",
      "0.0\n",
      "700\n",
      "nan\n",
      "0.0\n",
      "800\n",
      "nan\n",
      "0.0\n",
      "900\n",
      "nan\n",
      "0.0\n"
     ]
    }
   ],
   "source": [
    "Learning_Rate = 0.05\n",
    "\n",
    "cost_ = -y*tf.log(answer)-(1-y)*tf.log((1-answer))\n",
    "cost = tf.reduce_sum(cost_, reduction_indices=1)\n",
    "cost = tf.reduce_mean(cost, reduction_indices=0)\n",
    "train = tf.train.GradientDescentOptimizer(Learning_Rate).minimize(cost)\n",
    "\n",
    "comp_pred = tf.equal( tf.arg_max(answer, 1), tf.arg_max(y, 1) )\n",
    "accuracy = tf.reduce_mean(tf.cast(comp_pred, tf.float32))\n",
    "\n",
    "sess = tf.Session()\n",
    "init = tf.global_variables_initializer()\n",
    "sess.run(init) \n",
    "\n",
    "for i in range(1000):\n",
    "    _ , loss, acc = sess.run([train, cost, accuracy], feed_dict = tensor_map)\n",
    "    if i%100 == 0:\n",
    "        saver.save(sess, './tensorflow_live.ckpt')\n",
    "        print (i)\n",
    "        print (loss)\n",
    "        print (acc)\n",
    "    \n",
    "sess.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
