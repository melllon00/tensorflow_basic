{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## - 학습 3은 학습 2를 기반으로 진행합니다 (중반부터 학습 3 진행)\n",
    "\n",
    "## tensorflow 모델 생성하기\n",
    "**출처 ** <br>\n",
    "- https://www.youtube.com/watch?v=NZz2yq0G5LU\n",
    "- http://bcho.tistory.com/1154"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from tensorflow.examples.tutorials.mnist import input_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### input과 label 데이터 설정\n",
    "** 먼저 간단한 모델을 살펴보자 ** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "input_data = [[1,5,3,7,8,10,12],\n",
    "              [5,8,10,3,9,7,1]]\n",
    "label_data = [[0,0,0,1,0],\n",
    "              [1,0,0,0,0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "INPUT_SIZE = 7 \n",
    "HIDDEN_SIZE_1 = 10\n",
    "HIDDEN_SIZE_2 = 8\n",
    "CLASSES = 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** shape 에서 첫 번째 요소는 batch의 크기. 일반적으로 잘 모르기 때문에 None으로 써도 된다. **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x = tf.placeholder(tf.float32, shape = [None, INPUT_SIZE] )\n",
    "y = tf.placeholder(tf.float32, shape = [None, CLASSES] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tensor_map = {x : input_data, y : label_data}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** input weight 정의 ** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "weight_hidden_1 = tf.Variable( tf.truncated_normal(shape=[INPUT_SIZE, HIDDEN_SIZE_1]) , dtype=tf.float32 ) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** bias 정의 ** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bias_hidden_1 = tf.Variable( tf.zeros(shape=[HIDDEN_SIZE_1]) , dtype=tf.float32 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'MatMul:0' shape=(?, 10) dtype=float32>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.matmul( x , weight_hidden_1 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"Placeholder:0\", shape=(?, 7), dtype=float32)\n",
      "Tensor(\"Variable/read:0\", shape=(7, 10), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "print (x)\n",
    "print (weight_hidden_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "hidden_1 = tf.matmul( x , weight_hidden_1 ) + bias_hidden_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "weight_hidden_2 = tf.Variable( tf.truncated_normal(shape=[HIDDEN_SIZE_1, HIDDEN_SIZE_2]) , dtype=tf.float32 ) \n",
    "bias_hidden_2 = tf.Variable( tf.zeros(shape=[HIDDEN_SIZE_2]) , dtype=tf.float32 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "hidden_2 = tf.matmul( hidden_1 , weight_hidden_2 ) + bias_hidden_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "weight_output = tf.Variable( tf.truncated_normal(shape=[HIDDEN_SIZE_2, CLASSES]) , dtype=tf.float32 ) \n",
    "bias_output = tf.Variable( tf.zeros(shape=[CLASSES]) , dtype=tf.float32 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "answer = tf.matmul( hidden_2 , weight_output ) + bias_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** 위의 내용을 한 번에 모아서 해보자 ** \n",
    "\n",
    "신경망을 미리 설계해 놓고 아래에서 연산을 수행하여 답을 도출해낸다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "weight_hidden_1 = tf.Variable( tf.truncated_normal(shape=[INPUT_SIZE, HIDDEN_SIZE_1]) , dtype=tf.float32 ) \n",
    "bias_hidden_1 = tf.Variable( tf.zeros(shape=[HIDDEN_SIZE_1]) , dtype=tf.float32 )\n",
    "\n",
    "weight_hidden_2 = tf.Variable( tf.truncated_normal(shape=[HIDDEN_SIZE_1, HIDDEN_SIZE_2]) , dtype=tf.float32 ) \n",
    "bias_hidden_2 = tf.Variable( tf.zeros(shape=[HIDDEN_SIZE_2]) , dtype=tf.float32 )\n",
    "\n",
    "weight_output = tf.Variable( tf.truncated_normal(shape=[HIDDEN_SIZE_2, CLASSES]) , dtype=tf.float32 ) \n",
    "bias_output = tf.Variable( tf.zeros(shape=[CLASSES]) , dtype=tf.float32 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "hidden_1 = tf.matmul( x , weight_hidden_1 ) + bias_hidden_1\n",
    "hidden_2 = tf.matmul( hidden_1 , weight_hidden_2 ) + bias_hidden_2\n",
    "answer = tf.matmul( hidden_2 , weight_output ) + bias_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 구조\n",
    "    Input -  ㅁ  - ㅁ  - ㅁ - ㅁ   - ㅁ - ㅁ  - answer\n",
    "             |     |    |    |     |    |\n",
    "             w1   b2    w2   b2    wo   bo\n",
    "             \n",
    "    \n",
    "    인풋   히든1  2  결과  \n",
    "\n",
    "    7     10    8   5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"Placeholder:0\", shape=(?, 7), dtype=float32)\n",
      "Tensor(\"add_3:0\", shape=(?, 10), dtype=float32)\n",
      "Tensor(\"add_4:0\", shape=(?, 8), dtype=float32)\n",
      "Tensor(\"add_5:0\", shape=(?, 5), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "print (x)\n",
    "print (hidden_1)\n",
    "print (hidden_2)\n",
    "print (answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"Variable_6/read:0\", shape=(7, 10), dtype=float32)\n",
      "Tensor(\"Variable_7/read:0\", shape=(10,), dtype=float32)\n",
      "Tensor(\"Variable_8/read:0\", shape=(10, 8), dtype=float32)\n",
      "Tensor(\"Variable_9/read:0\", shape=(8,), dtype=float32)\n",
      "Tensor(\"Variable_10/read:0\", shape=(8, 5), dtype=float32)\n",
      "Tensor(\"Variable_11/read:0\", shape=(5,), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "print (weight_hidden_1)\n",
    "print (bias_hidden_1)\n",
    "\n",
    "print (weight_hidden_2)\n",
    "print (bias_hidden_2)\n",
    "\n",
    "print (weight_output)\n",
    "print (bias_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** sigmoid  함수 적용 법 ** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "weight_hidden_1 = tf.Variable( tf.truncated_normal(shape=[INPUT_SIZE, HIDDEN_SIZE_1]) , dtype=tf.float32 ) \n",
    "bias_hidden_1 = tf.Variable( tf.zeros(shape=[HIDDEN_SIZE_1]) , dtype=tf.float32 )\n",
    "hidden_1 = tf.sigmoid(tf.matmul( x , weight_hidden_1 ) + bias_hidden_1)\n",
    "\n",
    "weight_hidden_2 = tf.Variable( tf.truncated_normal(shape=[HIDDEN_SIZE_1, HIDDEN_SIZE_2]) , dtype=tf.float32 ) \n",
    "bias_hidden_2 = tf.Variable( tf.zeros(shape=[HIDDEN_SIZE_2]) , dtype=tf.float32 )\n",
    "hidden_2 = tf.sigmoid(tf.matmul( hidden_1 , weight_hidden_2 ) + bias_hidden_2)\n",
    "\n",
    "weight_output = tf.Variable( tf.truncated_normal(shape=[HIDDEN_SIZE_2, CLASSES]) , dtype=tf.float32 ) \n",
    "bias_output = tf.Variable( tf.zeros(shape=[CLASSES]) , dtype=tf.float32 )\n",
    "\n",
    "answer = tf.sigmoid(tf.matmul( hidden_2 , weight_output ) + bias_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** cost 엔트로피 ** ( tip : 매뉴얼을 항상 자주 보자 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cost = -y*tf.log(answer)-(1-y)*tf.log((1-answer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.08700245,  0.08184471,  1.83177435,  0.94000405,  0.57135564],\n",
       "       [ 2.02943397,  0.17305857,  1.6006968 ,  0.26148587,  0.74627531]], dtype=float32)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sess = tf.Session()\n",
    "init = tf.global_variables_initializer()\n",
    "sess.run(init)\n",
    "sess.run(cost, feed_dict = tensor_map)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** recude sum 과 reduce mean 사용해보기 **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8.2579012"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cost = tf.reduce_sum(-y*tf.log(answer)-(1-y)*tf.log((1-answer)))\n",
    "\n",
    "sess = tf.Session()\n",
    "init = tf.global_variables_initializer()\n",
    "sess.run(init)\n",
    "sess.run(cost, feed_dict = tensor_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8043766"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cost = tf.reduce_mean(-y*tf.log(answer)-(1-y)*tf.log((1-answer)))\n",
    "\n",
    "sess = tf.Session()\n",
    "init = tf.global_variables_initializer()\n",
    "sess.run(init)\n",
    "sess.run(cost, feed_dict = tensor_map)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** 수행 ** \n",
    "- cost를 최소화 하는 방향으로 최적화 해야 되니까 gradient descent가 필요.\n",
    "- learning rate 사용\n",
    "\n",
    "\n",
    "\n",
    "#### 조대협님의 블로그에서 가져온 내용 (Mnist data를 기준으로 한다. 자세한 내용은 링크 참조)\n",
    "\n",
    "우리가 구하고자 하는 값은 x 값으로 학습을 시켜서 0~9를 가장 잘 구별해내는 W와 b의 값을 찾는 일이다. \n",
    "\n",
    "여기서 코드를 주의깊게 봤다면 하나의 의문이 생길것이다.\n",
    "\n",
    "x의 데이타는 총 55000개로, 55000x784 행렬이 되고, W는 784x10 행렬이다. 이 둘을 곱하면, 55000x10 행렬이 되는데, b는 1x10 행렬로 차원이 달라서 합이 되지 않는다. \n",
    "\n",
    "텐서플로우와 파이썬에서는 이렇게 차원이 다른 행렬을 큰 행렬의 크기로 늘려주는 기능이 있는데, 이를 브로드 캐스팅이라고 한다. (브로드 캐스팅 개념 참고 - http://bcho.tistory.com/1153)\n",
    "\n",
    "브로드 캐스팅에 의해서 b는 55000x10 사이즈로 자동으로 늘어나고 각 행에는 첫행과 같은 데이타들로 채워지게 된다. \n",
    "\n",
    "소프트맥스 알고리즘을 이해하고 사용해도 좋지만, 텐서플로우에는 이미 tf.nn.softmax 라는 함수로 만들어져 있고, 대부분 많이 알려진 머신러닝 모델들은 샘플들이 많이 있기 때문에, 대략적인 원리만 이해하고 가져다 쓰는 것을 권장한다.\n",
    "\n",
    "보통 모델을 다 이해하려고 하다가 수학에서 부딪혀서 포기하는 경우가 많은데, 디테일한 모델을 이해하기 힘들면, 그냥 함수나 예제코드를 가져다 쓰는 방법으로 접근하자. \n",
    "\n",
    "우리가 일반적인 프로그래밍에서도 해쉬테이블이나 트리와 같은 자료구조에 대해서 대략적인 개념만 이해하고 미리 정의된 라이브러리를 사용하지 직접 해쉬 테이블등을 구현하는 경우는 드물다.\n",
    "\n",
    "\n",
    "### 코스트(비용) 함수\n",
    "\n",
    "이 소프트맥스 함수에 대한 코스트 함수는 크로스엔트로피 (Cross entropy) 함수의 평균을 이용하는데, 복잡한 산식 없이 그냥 외워서 쓰자. \n",
    "다행이도 크로스엔트로피 함수역시 함수로 구현이 되어있다.\n",
    "\n",
    "** Cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(tf.matmul(x, W) + b, y_)) ** \n",
    "\n",
    "가설에 의해 계산된 값 y를 넣지 않고 tf.matmul(x, W) + b 를 넣은 이유는  tf.nn.softmax_cross_entropy_with_logits 함수 자체가 softmax를 포함하기 때문이다. y_은 학습을 위해서 입력된 값이다.  \n",
    "\n",
    "\n",
    "-> 코스트 함수로 위 함수는 써 본 적은 없고, 자세한 내용은 저도 잘 모르겠으나 일단 사용해보겠습니다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[None, 1.1322856]\n",
      "Step :  0\n",
      "[None, 1.1105931]\n",
      "Step :  1\n",
      "[None, 1.0896574]\n",
      "Step :  2\n",
      "[None, 1.069412]\n",
      "Step :  3\n",
      "[None, 1.049744]\n",
      "Step :  4\n",
      "[None, 1.0305212]\n",
      "Step :  5\n",
      "[None, 1.0116106]\n",
      "Step :  6\n",
      "[None, 0.99288923]\n",
      "Step :  7\n",
      "[None, 0.97425824]\n",
      "Step :  8\n",
      "[None, 0.9556694]\n",
      "Step :  9\n"
     ]
    }
   ],
   "source": [
    "Learning_Rate = 0.05\n",
    "\n",
    "cost = tf.reduce_mean(-y*tf.log(answer)-(1-y)*tf.log((1-answer)))\n",
    "train = tf.train.GradientDescentOptimizer(Learning_Rate).minimize(cost)\n",
    "\n",
    "sess = tf.Session()\n",
    "init = tf.global_variables_initializer()\n",
    "sess.run(init)\n",
    "\n",
    "for i in range(10):\n",
    "    print (sess.run([train, cost], feed_dict = tensor_map))\n",
    "    print (\"Step : \",i )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** 학습 100 번 수행 ** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[None, 0.93716115]\n",
      "Step :  0\n",
      "[None, 0.91888553]\n",
      "Step :  1\n",
      "[None, 0.90108842]\n",
      "Step :  2\n",
      "[None, 0.88402575]\n",
      "Step :  3\n",
      "[None, 0.86786509]\n",
      "Step :  4\n",
      "[None, 0.8526473]\n",
      "Step :  5\n",
      "[None, 0.83831531]\n",
      "Step :  6\n",
      "[None, 0.82476711]\n",
      "Step :  7\n",
      "[None, 0.81189477]\n",
      "Step :  8\n",
      "[None, 0.79960364]\n",
      "Step :  9\n",
      "[None, 0.78781605]\n",
      "Step :  10\n",
      "[None, 0.77647078]\n",
      "Step :  11\n",
      "[None, 0.76552039]\n",
      "Step :  12\n",
      "[None, 0.75492728]\n",
      "Step :  13\n",
      "[None, 0.74466175]\n",
      "Step :  14\n",
      "[None, 0.7346999]\n",
      "Step :  15\n",
      "[None, 0.7250222]\n",
      "Step :  16\n",
      "[None, 0.71561205]\n",
      "Step :  17\n",
      "[None, 0.70645541]\n",
      "Step :  18\n",
      "[None, 0.69754016]\n",
      "Step :  19\n",
      "[None, 0.68885505]\n",
      "Step :  20\n",
      "[None, 0.68039048]\n",
      "Step :  21\n",
      "[None, 0.67213714]\n",
      "Step :  22\n",
      "[None, 0.66408694]\n",
      "Step :  23\n",
      "[None, 0.65623158]\n",
      "Step :  24\n",
      "[None, 0.64856368]\n",
      "Step :  25\n",
      "[None, 0.64107591]\n",
      "Step :  26\n",
      "[None, 0.63376105]\n",
      "Step :  27\n",
      "[None, 0.62661254]\n",
      "Step :  28\n",
      "[None, 0.61962378]\n",
      "Step :  29\n",
      "[None, 0.6127882]\n",
      "Step :  30\n",
      "[None, 0.60609972]\n",
      "Step :  31\n",
      "[None, 0.59955251]\n",
      "Step :  32\n",
      "[None, 0.59314096]\n",
      "Step :  33\n",
      "[None, 0.58686018]\n",
      "Step :  34\n",
      "[None, 0.58070558]\n",
      "Step :  35\n",
      "[None, 0.57467395]\n",
      "Step :  36\n",
      "[None, 0.56876218]\n",
      "Step :  37\n",
      "[None, 0.56296927]\n",
      "Step :  38\n",
      "[None, 0.55729473]\n",
      "Step :  39\n",
      "[None, 0.55174035]\n",
      "Step :  40\n",
      "[None, 0.5463084]\n",
      "Step :  41\n",
      "[None, 0.54100311]\n",
      "Step :  42\n",
      "[None, 0.53582913]\n",
      "Step :  43\n",
      "[None, 0.5307914]\n",
      "Step :  44\n",
      "[None, 0.52589446]\n",
      "Step :  45\n",
      "[None, 0.52114213]\n",
      "Step :  46\n",
      "[None, 0.51653636]\n",
      "Step :  47\n",
      "[None, 0.51207763]\n",
      "Step :  48\n",
      "[None, 0.5077644]\n",
      "Step :  49\n",
      "[None, 0.50359321]\n",
      "Step :  50\n",
      "[None, 0.4995594]\n",
      "Step :  51\n",
      "[None, 0.49565735]\n",
      "Step :  52\n",
      "[None, 0.49188066]\n",
      "Step :  53\n",
      "[None, 0.48822254]\n",
      "Step :  54\n",
      "[None, 0.48467636]\n",
      "Step :  55\n",
      "[None, 0.48123559]\n",
      "Step :  56\n",
      "[None, 0.47789398]\n",
      "Step :  57\n",
      "[None, 0.47464561]\n",
      "Step :  58\n",
      "[None, 0.47148538]\n",
      "Step :  59\n",
      "[None, 0.46840811]\n",
      "Step :  60\n",
      "[None, 0.46540934]\n",
      "Step :  61\n",
      "[None, 0.46248493]\n",
      "Step :  62\n",
      "[None, 0.45963106]\n",
      "Step :  63\n",
      "[None, 0.45684448]\n",
      "Step :  64\n",
      "[None, 0.45412198]\n",
      "Step :  65\n",
      "[None, 0.4514606]\n",
      "Step :  66\n",
      "[None, 0.44885769]\n",
      "Step :  67\n",
      "[None, 0.446311]\n",
      "Step :  68\n",
      "[None, 0.44381824]\n",
      "Step :  69\n",
      "[None, 0.44137725]\n",
      "Step :  70\n",
      "[None, 0.43898621]\n",
      "Step :  71\n",
      "[None, 0.43664327]\n",
      "Step :  72\n",
      "[None, 0.43434677]\n",
      "Step :  73\n",
      "[None, 0.43209523]\n",
      "Step :  74\n",
      "[None, 0.42988712]\n",
      "Step :  75\n",
      "[None, 0.42772102]\n",
      "Step :  76\n",
      "[None, 0.4255957]\n",
      "Step :  77\n",
      "[None, 0.42350993]\n",
      "Step :  78\n",
      "[None, 0.42146248]\n",
      "Step :  79\n",
      "[None, 0.41945225]\n",
      "Step :  80\n",
      "[None, 0.41747823]\n",
      "Step :  81\n",
      "[None, 0.41553935]\n",
      "Step :  82\n",
      "[None, 0.41363472]\n",
      "Step :  83\n",
      "[None, 0.41176343]\n",
      "Step :  84\n",
      "[None, 0.40992445]\n",
      "Step :  85\n",
      "[None, 0.40811712]\n",
      "Step :  86\n",
      "[None, 0.40634051]\n",
      "Step :  87\n",
      "[None, 0.40459388]\n",
      "Step :  88\n",
      "[None, 0.40287656]\n",
      "Step :  89\n",
      "[None, 0.40118772]\n",
      "Step :  90\n",
      "[None, 0.39952669]\n",
      "Step :  91\n",
      "[None, 0.39789283]\n",
      "Step :  92\n",
      "[None, 0.39628547]\n",
      "Step :  93\n",
      "[None, 0.39470392]\n",
      "Step :  94\n",
      "[None, 0.39314777]\n",
      "Step :  95\n",
      "[None, 0.39161617]\n",
      "Step :  96\n",
      "[None, 0.39010882]\n",
      "Step :  97\n",
      "[None, 0.388625]\n",
      "Step :  98\n",
      "[None, 0.38716426]\n",
      "Step :  99\n"
     ]
    }
   ],
   "source": [
    "for i in range(100):\n",
    "    print (sess.run([train, cost], feed_dict = tensor_map))\n",
    "    print (\"Step : \",i )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sess.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    " <br><br>\n",
    "\n",
    "## 학습 3 시작 \n",
    "\n",
    "### cost 에서 reduce_mean 을 제거하고 수행해보자\n",
    "\n",
    "** reduce_mean은 모든 차원의 값을 제거하고 나서 하나의 값으로 나타낸다 ** \n",
    "\n",
    "    1 2 3 4\n",
    "    5 6 7 8 \n",
    "\n",
    "reduce_mean은 위와 같은 배열이 있으면 (1+2+3+4+5+6+7+8)/8 연산을 통해서 모든 차원을 제거하고 하나의 값만 출력해준다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.14097062  1.50098753  0.28970671  0.22760819  2.3666749 ]\n",
      " [ 1.57549131  2.54993439  0.26265255  1.0441227   2.69189954]]\n",
      "\n",
      "[[ 0.14097062  1.50098753  0.28970671  0.22760819  2.3666749 ]\n",
      " [ 1.57549131  2.54993439  0.26265255  1.0441227   2.69189954]]\n",
      "[[ 0.14097062  1.50098753  0.28970671  0.22760819  2.3666749 ]\n",
      " [ 1.57549131  2.54993439  0.26265255  1.0441227   2.69189954]]\n",
      "[[ 0.14097062  1.50098753  0.28970671  0.22760819  2.3666749 ]\n",
      " [ 1.57549131  2.54993439  0.26265255  1.0441227   2.69189954]]\n",
      "[[ 0.14097062  1.50098753  0.28970671  0.22760819  2.3666749 ]\n",
      " [ 1.57549131  2.54993439  0.26265255  1.0441227   2.69189954]]\n",
      "[[ 0.14097062  1.50098753  0.28970671  0.22760819  2.3666749 ]\n",
      " [ 1.57549131  2.54993439  0.26265255  1.0441227   2.69189954]]\n",
      "[[ 0.14097062  1.50098753  0.28970671  0.22760819  2.3666749 ]\n",
      " [ 1.57549131  2.54993439  0.26265255  1.0441227   2.69189954]]\n",
      "[[ 0.14097062  1.50098753  0.28970671  0.22760819  2.3666749 ]\n",
      " [ 1.57549131  2.54993439  0.26265255  1.0441227   2.69189954]]\n",
      "[[ 0.14097062  1.50098753  0.28970671  0.22760819  2.3666749 ]\n",
      " [ 1.57549131  2.54993439  0.26265255  1.0441227   2.69189954]]\n",
      "[[ 0.14097062  1.50098753  0.28970671  0.22760819  2.3666749 ]\n",
      " [ 1.57549131  2.54993439  0.26265255  1.0441227   2.69189954]]\n",
      "[[ 0.14097062  1.50098753  0.28970671  0.22760819  2.3666749 ]\n",
      " [ 1.57549131  2.54993439  0.26265255  1.0441227   2.69189954]]\n"
     ]
    }
   ],
   "source": [
    "Learning_Rate = 0.05\n",
    "\n",
    "cost = -y*tf.log(answer)-(1-y)*tf.log((1-answer))\n",
    "train = tf.train.GradientDescentOptimizer(Learning_Rate).minimize(cost)\n",
    "\n",
    "sess = tf.Session()\n",
    "init = tf.global_variables_initializer()\n",
    "sess.run(init)\n",
    "\n",
    "loss = sess.run(cost, feed_dict = tensor_map)\n",
    "print (loss) \n",
    "print ()    \n",
    "\n",
    "for i in range(10):\n",
    "    loss = sess.run(cost, feed_dict = tensor_map)\n",
    "    print (loss)\n",
    "\n",
    "sess.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**   https://www.tensorflow.org/api_docs/python/tf/reduce_mean 에서 확인해보세요  ** \n",
    "\n",
    "** 예시 (For example): ** "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "    x = tf.constant([[1., 1.], [2., 2.]])\n",
    "    tf.reduce_mean(x)  # 1.5\n",
    "    tf.reduce_mean(x, 0)  # [1.5, 1.5]\n",
    "    tf.reduce_mean(x, 1)  # [1.,  2.]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** 비교하기 쉽게 출력 해보자 ** \n",
    "\n",
    "- 차원의 차이가 아래와 같이 난다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.22220834  1.42398429  1.05175662  0.19866461  0.28180537]\n",
      " [ 1.94174027  1.80546308  0.92534113  1.96047449  0.31566393]]\n",
      "[ 3.17841935  6.94868279]\n"
     ]
    }
   ],
   "source": [
    "Learning_Rate = 0.05\n",
    "\n",
    "cost_ = -y*tf.log(answer)-(1-y)*tf.log((1-answer))\n",
    "cost = tf.reduce_sum(cost_, reduction_indices=1)\n",
    "train = tf.train.GradientDescentOptimizer(Learning_Rate).minimize(cost)\n",
    "\n",
    "sess = tf.Session()\n",
    "init = tf.global_variables_initializer()\n",
    "sess.run(init) \n",
    "\n",
    "loss_, loss = sess.run([cost_, cost], feed_dict = tensor_map)\n",
    "print (loss_)\n",
    "print (loss)\n",
    "    \n",
    "sess.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "- 배열에서 원소 하나 하나는 함수와 가지는 loss를 의미한다\n",
    "\n",
    "- 따라서, 함수 하나와 모든 점들간의 loss를 보기 위해서 모든 파라미터에 대하여 평균을 구하거나 합을 구하는 reduce를 사용"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 위에서 reduce_sum 을 구했으니까, reduce_mean이 필요\n",
    "\n",
    "- reduce_mean을 해주면 차원이 또 줄어든다 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "4.08091\n",
      "100\n",
      "3.77948\n",
      "200\n",
      "3.52982\n",
      "300\n",
      "3.32602\n",
      "400\n",
      "3.15119\n",
      "500\n",
      "2.99706\n",
      "600\n",
      "2.86232\n",
      "700\n",
      "2.74475\n",
      "800\n",
      "2.641\n",
      "900\n",
      "2.54821\n"
     ]
    }
   ],
   "source": [
    "Learning_Rate = 0.05\n",
    "\n",
    "cost_ = -y*tf.log(answer)-(1-y)*tf.log((1-answer))\n",
    "cost = tf.reduce_sum(cost_, reduction_indices=1)\n",
    "cost = tf.reduce_mean(cost, reduction_indices=0)\n",
    "train = tf.train.GradientDescentOptimizer(Learning_Rate).minimize(cost)\n",
    "\n",
    "sess = tf.Session()\n",
    "init = tf.global_variables_initializer()\n",
    "sess.run(init) \n",
    "\n",
    "for i in range(1000):\n",
    "    if i%100 == 0:\n",
    "        _ , loss = sess.run([train, cost], feed_dict = tensor_map)\n",
    "        print (i)\n",
    "        print (loss)\n",
    "    \n",
    "sess.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### accuracy를 구해보자!! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Learning_Rate = 0.05\n",
    "\n",
    "cost_ = -y*tf.log(answer)-(1-y)*tf.log((1-answer))\n",
    "cost = tf.reduce_sum(cost_, reduction_indices=1)\n",
    "cost = tf.reduce_mean(cost, reduction_indices=0)\n",
    "train = tf.train.GradientDescentOptimizer(Learning_Rate).minimize(cost)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** train 해서 나온 answer 중에서 가장 큰 것과, label 과 비교한다. ** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "comp_pred = tf.equal( tf.arg_max(answer, 1), tf.arg_max(y, 1) )\n",
    "accuracy = tf.reduce_mean(tf.cast(comp_pred, tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "4.54525\n",
      "0.5\n",
      "\n",
      "100\n",
      "0.853118\n",
      "1.0\n",
      "\n",
      "200\n",
      "0.436865\n",
      "1.0\n",
      "\n",
      "300\n",
      "0.269513\n",
      "1.0\n",
      "\n",
      "400\n",
      "0.189196\n",
      "1.0\n",
      "\n",
      "500\n",
      "0.143947\n",
      "1.0\n",
      "\n",
      "600\n",
      "0.115434\n",
      "1.0\n",
      "\n",
      "700\n",
      "0.0960026\n",
      "1.0\n",
      "\n",
      "800\n",
      "0.0819866\n",
      "1.0\n",
      "\n",
      "900\n",
      "0.0714351\n",
      "1.0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sess = tf.Session()\n",
    "init = tf.global_variables_initializer()\n",
    "sess.run(init) \n",
    "\n",
    "for i in range(1000):\n",
    "    _ , loss, acc = sess.run([train, cost, accuracy], feed_dict = tensor_map)\n",
    "    if i%100 == 0: \n",
    "        print (i)\n",
    "        print (loss)\n",
    "        print (acc)\n",
    "        print ()\n",
    "    \n",
    "sess.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 텐서플로우는 체크포인트에 weight 정보를 넣는다. test 모델에서 파일에 있는 weight를 가져온다. \n",
    "- weight 의 아래 쪽에 saver를 만든다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "weight_hidden_1 = tf.Variable( tf.truncated_normal(shape=[INPUT_SIZE, HIDDEN_SIZE_1]) , dtype=tf.float32 ) \n",
    "bias_hidden_1 = tf.Variable( tf.zeros(shape=[HIDDEN_SIZE_1]) , dtype=tf.float32 )\n",
    "\n",
    "weight_hidden_2 = tf.Variable( tf.truncated_normal(shape=[HIDDEN_SIZE_1, HIDDEN_SIZE_2]) , dtype=tf.float32 ) \n",
    "bias_hidden_2 = tf.Variable( tf.zeros(shape=[HIDDEN_SIZE_2]) , dtype=tf.float32 )\n",
    "\n",
    "weight_output = tf.Variable( tf.truncated_normal(shape=[HIDDEN_SIZE_2, CLASSES]) , dtype=tf.float32 ) \n",
    "bias_output = tf.Variable( tf.zeros(shape=[CLASSES]) , dtype=tf.float32 )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** 아래의 영역을 추가해준다 ** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "param_list = [weight_hidden_1, bias_hidden_1, weight_hidden_2, bias_hidden_2, weight_output, bias_output ]\n",
    "saver = tf.train.Saver(param_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "hidden_1 = tf.sigmoid(tf.matmul( x , weight_hidden_1 ) + bias_hidden_1)\n",
    "hidden_2 = tf.sigmoid(tf.matmul( hidden_1 , weight_hidden_2 ) + bias_hidden_2)\n",
    "answer = tf.sigmoid(tf.matmul( hidden_2 , weight_output ) + bias_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** saver.save 함수를 통하여 파일이 생성됨  ** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "4.40292\n",
      "0.0\n",
      "100\n",
      "0.755229\n",
      "1.0\n",
      "200\n",
      "0.393953\n",
      "1.0\n",
      "300\n",
      "0.25012\n",
      "1.0\n",
      "400\n",
      "0.177993\n",
      "1.0\n",
      "500\n",
      "0.136179\n",
      "1.0\n",
      "600\n",
      "0.109399\n",
      "1.0\n",
      "700\n",
      "0.0909777\n",
      "1.0\n",
      "800\n",
      "0.0776153\n",
      "1.0\n",
      "900\n",
      "0.0675219\n",
      "1.0\n"
     ]
    }
   ],
   "source": [
    "Learning_Rate = 0.05\n",
    "\n",
    "cost_ = -y*tf.log(answer)-(1-y)*tf.log((1-answer))\n",
    "cost = tf.reduce_sum(cost_, reduction_indices=1)\n",
    "cost = tf.reduce_mean(cost, reduction_indices=0)\n",
    "train = tf.train.GradientDescentOptimizer(Learning_Rate).minimize(cost)\n",
    "\n",
    "comp_pred = tf.equal( tf.arg_max(answer, 1), tf.arg_max(y, 1) )\n",
    "accuracy = tf.reduce_mean(tf.cast(comp_pred, tf.float32))\n",
    "\n",
    "sess = tf.Session()\n",
    "init = tf.global_variables_initializer()\n",
    "sess.run(init) \n",
    "\n",
    "for i in range(1000):\n",
    "    _ , loss, acc = sess.run([train, cost, accuracy], feed_dict = tensor_map)\n",
    "    if i%100 == 0:\n",
    "        saver.save(sess, './tensorflow_live.ckpt')\n",
    "        print (i)\n",
    "        print (loss)\n",
    "        print (acc)\n",
    "    \n",
    "sess.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
